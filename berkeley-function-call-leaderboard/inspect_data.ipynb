{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "from pprint import pp\n",
    "\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(path):\n",
    "    dirs = [dir for dir in os.listdir(path) if os.path.isdir(os.path.join(path, dir))]\n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    lines = []\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_dir(model_dirs, gen_mode, model_name_escaped, dedup_str, n_tool_calls):\n",
    "    for model_dir in model_dirs:\n",
    "        if gen_mode == \"structured\" and \"unstructured\" in model_dir:\n",
    "            continue\n",
    "\n",
    "        if gen_mode in model_dir and model_name_escaped in model_dir and dedup_str in model_dir and n_tool_calls in model_dir:\n",
    "            return model_dir\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_it(idx, questions, solutions, generations, scores, idx_type=\"error\"):\n",
    "\n",
    "    if idx_type == \"absolute\":\n",
    "        question = questions[idx]\n",
    "        solution = solutions[idx]\n",
    "        generation = generations[idx]\n",
    "        score = [score for i, score in enumerate(scores) if i > 0 and score[\"id\"] == idx]\n",
    "    elif idx_type == \"error\":\n",
    "        if idx+1 > len(scores):\n",
    "            raise ValueError(f\"There are {len(scores)-1} errors in this file. But you requested error {idx} (0-index).\")\n",
    "        score = scores[idx+1]\n",
    "        abs_idx = score[\"id\"] - 1\n",
    "        question = questions[abs_idx]\n",
    "        solution = solutions[abs_idx]\n",
    "        generation = generations[abs_idx]\n",
    "    else:\n",
    "        raise ValueError(\"Wrong idx_type.\")\n",
    "\n",
    "    return question, solution, generation, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values( question, solution, generation, score):\n",
    "    user_query = question[\"question\"]\n",
    "    tools = question[\"function\"]\n",
    "    if not isinstance(tools, list):\n",
    "        tools = [tools]\n",
    "\n",
    "    raw_result = generation[\"result\"]\n",
    "    messages = generation[\"messages\"]\n",
    "    tool_calls = generation[\"tool_calls\"]\n",
    "    n_tool_calls = generation[\"n_tool_calls\"]\n",
    "    valid = score[\"valid\"]\n",
    "    error = score[\"error\"]\n",
    "    error_type = score[\"error_type\"]\n",
    "    decoded_result = score[\"model_result_decoded\"]\n",
    "\n",
    "    result = {\n",
    "        \"user_query\": user_query,\n",
    "        \"tools\": tools,\n",
    "        \"valid\": valid,\n",
    "        \"tool_calls\": tool_calls,\n",
    "        \"solution\": solution,\n",
    "        \"error\": error,\n",
    "        \"error_type\": error_type,\n",
    "        \"raw_result\": raw_result,\n",
    "        \"decoded_result\": decoded_result,\n",
    "        \"messages\": messages,\n",
    "        \"n_tool_calls\": n_tool_calls\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spec_to_result(spec, idx, idx_type):\n",
    "\n",
    "    # Unpack spec\n",
    "    bfcl_category = spec[\"bfcl_category\"]\n",
    "    model_name = spec[\"model_name\"]\n",
    "    dedup = spec[\"dedup\"]\n",
    "    out_dir = spec[\"out_dir\"]\n",
    "    gen_mode = spec[\"gen_mode\"]\n",
    "    n_tool_calls = spec[\"n_tool_calls\"]\n",
    "\n",
    "    # Get paths\n",
    "    test_file = f\"gorilla_openfunctions_v1_test_{bfcl_category}.json\"\n",
    "    questions_path = os.path.join(\"./data\", test_file)\n",
    "    solutions_path = os.path.join(\"./data/possible_answer\", test_file)\n",
    "\n",
    "    model_name_escaped = model_name.replace(\"/\", \"_\")\n",
    "    dedup_str = \"dedup\" if dedup else \"\"\n",
    "    model_dirs = get_directories(out_dir)\n",
    "    model_dir = get_model_dir(model_dirs, gen_mode, model_name_escaped, dedup_str, n_tool_calls)\n",
    "    generations_path = os.path.join(out_dir, model_dir, \"generations\", test_file)\n",
    "    scores_path = os.path.join(out_dir, model_dir, \"scores\", bfcl_category + \"_score.json\")\n",
    "\n",
    "    # Load all files\n",
    "    questions = load_jsonl(questions_path)\n",
    "    solutions = load_jsonl(solutions_path)\n",
    "    generations = load_jsonl(generations_path)\n",
    "    scores = load_jsonl(scores_path)\n",
    "\n",
    "    # Index into the exact question, solution, generation, score\n",
    "    question, solution, generation, score = index_it(idx, questions, solutions, generations, scores, idx_type=idx_type)\n",
    "    values = extract_values(question, solution, generation, score)\n",
    "\n",
    "    result = spec | values | {\"idx\": idx, \"idx_type\": idx_type}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretty Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_result(result, width=200, verbose=0):\n",
    "    print(\"-\"*120)\n",
    "    dedup_str = \"-dedup\" if dedup else \"\"\n",
    "    print(f\"Example {result['idx']} of {result['bfcl_category']} BFCL\\t\\t\\t\\t\\t({result['model_name']}-{result['gen_mode']} with {result['n_tool_calls']}{dedup_str})\")\n",
    "    print(\"-\"*120, end=\"\\n\\n\")\n",
    "\n",
    "    color_map = {}\n",
    "    print(colored(f\"User Query:\\n{result['user_query']}\", \"yellow\"), end=\"\\n\\n\")\n",
    "\n",
    "    print(colored(f\"Valid: {result['valid']}\", \"blue\"), end=\"\\n\\n\")\n",
    "\n",
    "    print(colored(f\"Error:\\n{result['error']}\\n{result['error_type']}\", \"blue\"), end=\"\\n\\n\")\n",
    "\n",
    "    print(colored(\"My Tool Calls:\", \"red\"))\n",
    "    for tool_call in result['tool_calls']:\n",
    "        print(colored(tool_call, \"red\"))\n",
    "    print()\n",
    "\n",
    "    print(colored(\"Correct Tool Calls:\", \"green\"))\n",
    "    for tool_name, tool_args in result['solution'].items():\n",
    "        print(colored(f\"{{'tool_name': '{tool_name}', 'tool_arguments': {tool_args}}}\", \"green\"))\n",
    "    print()\n",
    "\n",
    "    print(colored(f\"Raw Result:\\n{result['raw_result']}\", \"yellow\"), end=\"\\n\\n\")\n",
    "\n",
    "    if verbose >= 1:\n",
    "\n",
    "        print(colored(f\"Decoded Result:\\n{result['decoded_result']}\", \"blue\"), end=\"\\n\\n\")\n",
    "\n",
    "        print(\"Given Tool Calls:\")\n",
    "        for tool in result[\"tools\"]:\n",
    "            pp(tool, width=width)\n",
    "        print()\n",
    "\n",
    "    if verbose >= 2:\n",
    "        print(\"Messages:\")\n",
    "        pretty_print_conversation(result['messages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "bfcl_category = \"parallel_function\"\n",
    "\n",
    "dedup = False\n",
    "gen_mode = \"meta_tool\"\n",
    "n_tool_calls = \"auto\"\n",
    "model_name = \"databricks/dbrx-instruct\"\n",
    "out_dir = \"./outputs\"\n",
    "\n",
    "spec = {\n",
    "    \"bfcl_category\": bfcl_category,\n",
    "    \"gen_mode\": gen_mode,\n",
    "    \"n_tool_calls\": n_tool_calls,\n",
    "    \"model_name\": model_name,\n",
    "    \"dedup\": dedup,\n",
    "    \"out_dir\": out_dir,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Example 0 of parallel_function BFCL\t\t\t\t\t(databricks/dbrx-instruct-meta_tool with auto)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\u001b[33mUser Query:\n",
      "Calculate the resistance of a wire with a length of 5m and cross sectional area 0.01m² with resistivity of copper and aluminum\u001b[0m\n",
      "\n",
      "\u001b[34mValid: False\u001b[0m\n",
      "\n",
      "\u001b[34mError:\n",
      "['Wrong number of functions.']\n",
      "parallel_function_checker_no_order:wrong_count\u001b[0m\n",
      "\n",
      "\u001b[31mMy Tool Calls:\u001b[0m\n",
      "\u001b[31m{'tool_name': 'calculate_resistance', 'tool_arguments': {'length': 5, 'area': 0.01, 'resistivity': 'copper'}}\u001b[0m\n",
      "\n",
      "\u001b[32mCorrect Tool Calls:\u001b[0m\n",
      "\u001b[32m{'tool_name': 'calculate_resistance_1', 'tool_arguments': {'length': [5], 'area': [0.01], 'resistivity': ['copper']}}\u001b[0m\n",
      "\u001b[32m{'tool_name': 'calculate_resistance_2', 'tool_arguments': {'length': [5], 'area': [0.01], 'resistivity': ['aluminum']}}\u001b[0m\n",
      "\n",
      "\u001b[33mRaw Result:\n",
      "[calculate_resistance(length=5, area=0.01, resistivity='copper')]\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "result = spec_to_result(spec, idx, idx_type=\"error\")\n",
    "pp_result(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gorilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
