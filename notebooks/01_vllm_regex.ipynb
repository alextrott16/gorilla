{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export HF_HUB_ENABLE_HF_TRANSFER=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from functools import reduce\n",
    "\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from outlines import models, generate\n",
    "from outlines.fsm.json_schema import build_regex_from_schema, get_schema_from_signature\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def3fc6bbf7647f588f0c1cc9b99a988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d94221e628c473a85912ae9964b56b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90bbde45d6d4ae5bd93e64a9599b4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7704641e4d7e4f20927250107d9c2032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1544d36942e84d02b8c5cb385185e319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6b959de52749258f35b9a79f3aaa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4d00621a3a420e961f46ddb3189351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547a777ab9f842b3b6f99877586b761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b0e343d5aa4def9ac77919218d35ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5100a908a3164d07a914e4445de2e63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fb4308293d461c906320f8984b1ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2379222f1ed74d05925d3efa7a67af7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if mode == \"vllm-endpoint\":\n",
    "    client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"-\")\n",
    "elif mode == \"transformers\":\n",
    "    token = \"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\"\n",
    "    model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)\n",
    "    chat_template = \"{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '\\n[INST] ' + message['content'] + ' [/INST]' }}{% else %}{{ '\\n' + message['content'] + eos_token}}{% endif %}{% endfor %}\"\n",
    "    tokenizer.chat_template = chat_template\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        output_attentions=True,\n",
    "        token=token,\n",
    "    )\n",
    "    model = models.Transformers(llm, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool To Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_or(pattern1, pattern2):\n",
    "    return f\"(?:{pattern1}|{pattern2})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sometime_guide(regex_pattern, start_guided_pattern=\"<tool_call>\", end_guided_pattern=\"</tool_call>\"):\n",
    "    \"\"\"\n",
    "    Only do guided generation sometimes, in between start_word and end_word.\n",
    "    \"\"\"\n",
    "    return f\".*?(?={start_guided_pattern}){start_guided_pattern}({regex_pattern}).*?(?={end_guided_pattern}){end_guided_pattern}.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bfcl(tool):\n",
    "    return isinstance(tool, dict) and list(tool.keys()) == ['name', 'description', 'parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_pattern(pattern, num_repeats):\n",
    "    \"\"\"Repeat the regex pattern `pattern` `num_repeats` times.\n",
    "\n",
    "    If `num_repeats` is `None`, allow the pattern to be repeated an unlimited number of times.\n",
    "    If `num_repeats` is an integer, repeat the pattern exactly `num` times.\n",
    "    If `num_repeats` is an iterable with length two, repeat the pattern anywhere between `num[0]` and `num[1]` times, inclusive.\n",
    "    \"\"\"\n",
    "    if num_repeats is None:\n",
    "        result = f\"({pattern})*\"\n",
    "    elif isinstance(num_repeats, int):\n",
    "        result = f\"({pattern}){{{num_repeats}}}\"\n",
    "    elif isinstance(num_repeats, Union[list, tuple, set]) and len(num_repeats) == 2:\n",
    "        return f\"({pattern}){{{num_repeats[0]},{num_repeats[1]}}}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_to_regex(\n",
    "    tool,\n",
    "    n_tool_calls=1,\n",
    "    start_guided_pattern=\"<tool_call>\",\n",
    "    end_guided_pattern=\"</tool_call>\",\n",
    "    sometimes=False,\n",
    "    whitespace_pattern=None,\n",
    "    test_category=None,\n",
    "    ):\n",
    "\n",
    "    if isinstance(tool, list):\n",
    "        values = [\n",
    "            tool_to_regex(_tool, n_tool_calls=n_tool_calls, start_guided_pattern=start_guided_pattern, end_guided_pattern=end_guided_pattern, sometimes=sometimes, whitespace_pattern=whitespace_pattern, test_category=test_category,)\n",
    "            for _tool in tool\n",
    "            ]\n",
    "        regex_strs = [v[0] for v in values]\n",
    "        regex_str = reduce(regex_or, regex_strs)\n",
    "        schema_strs = [v[1] for v in values]\n",
    "        schema_str = \"\\n\".join(schema_strs)\n",
    "    elif is_bfcl(tool):\n",
    "        schema_str = bfcl_function_to_schema(tool, test_category).strip()\n",
    "        schema_regex = build_regex_from_schema(schema_str, whitespace_pattern)\n",
    "        regex_str = f'{{\"tool_name\": \"{tool[\"name\"]}\", \"tool_arguments\": {schema_regex}}}'\n",
    "    elif isinstance(tool, type(BaseModel)):\n",
    "        schema_json = tool.model_json_schema()\n",
    "        schema_str = json.dumps(schema_json).strip()\n",
    "        schema_regex = build_regex_from_schema(schema_str, whitespace_pattern)\n",
    "        regex_str = f'{{\"tool_name\": \"{schema_json[\"title\"]}\", \"tool_arguments\": {schema_regex}}}'\n",
    "    elif callable(tool):\n",
    "        schema_json = get_schema_from_signature(tool)\n",
    "        schema_str = json.dumps(schema_json).strip()\n",
    "        schema_regex = build_regex_from_schema(schema_str, whitespace_pattern)\n",
    "        regex_str = f'{{\"tool_name\": \"{tool.__name__}\", \"tool_arguments\": {schema_regex}}}'\n",
    "    elif isinstance(tool, str):\n",
    "        schema_str = re.sub(r'\\s+', ' ', tool).strip()\n",
    "        schema_regex = build_regex_from_schema(schema_str, whitespace_pattern)\n",
    "        regex_str = f'{{\"tool_name\": \"{json.loads(schema_str)[\"title\"]}\", \"tool_arguments\": {schema_regex}}}'\n",
    "\n",
    "    # if sometimes:\n",
    "    #     regex_str = sometime_guide(regex_str)\n",
    "    # elif not isinstance(tool, list):\n",
    "    #     regex_str = f\"{start_guided_pattern}{regex_str}{end_guided_pattern}\"\n",
    "\n",
    "    if not isinstance(tool, list):\n",
    "        regex_str = repeat_pattern(regex_str, n_tool_calls)\n",
    "\n",
    "    return regex_str, schema_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_prompt(\n",
    "    tool_schema,\n",
    "    tool_list_start=\"<tool>\",\n",
    "    tool_list_end=\"</tools>\",\n",
    "    tool_call_start=\"<tool_call>\",\n",
    "    tool_call_end=\"</tool_call>\",\n",
    "    tool_response_start=\"<tool_response>\",\n",
    "    tool_response_end=\"</tool_response>\"\n",
    "    ):\n",
    "\n",
    "    system_prompt = \"\"\"You are a function calling AI model. Your job is to answer the user's questions and you may call one or more functions to do this.\n",
    "\n",
    "\n",
    "    Please use your own judgment as to whether or not you should call a function. In particular, you must follow these guiding principles:\n",
    "    1. You may call one or more functions to assist with the user query.\n",
    "    2. You do not need to call a function. If none of the functions can be used to answer the user's question, please do not make the function call.\n",
    "    3. Don't make assumptions about what values to plug into functions. If you are missing the parameters to make a function call, please ask the user for the parameters.\n",
    "    4. You may assume the user has implemented the function themselves.\n",
    "    5. You may assume the user will call the function on their own. You should NOT ask the user to call the function and let you know the result; they will do this on their own.\n",
    "\n",
    "\n",
    "    You can only call functions according the following formatting rules:\n",
    "    Rule 1: All the functions you have access to are contained within {tool_list_start}{tool_list_end} XML tags. You cannot use any functions that are not listed between these tags.\n",
    "\n",
    "    Rule 2: For each function call return a json object (using quotes) with function name and arguments within {tool_call_start}\\n{{ }}\\n{tool_call_end} XML tags as follows:\n",
    "    * With arguments:\n",
    "    {tool_call_start}\\n{{\"tool_name\": \"function_name\", \"tool_arguments\": {{\"argument_1_name\": \"value\", \"argument_2_name\": \"value\"}} }}\\n{tool_call_end}\n",
    "    * Without arguments:\n",
    "    {tool_call_start}\\n{{ \"tool_name\": \"function_name\", \"tool_arguments\": {{}} }}\\n{tool_call_end}\n",
    "    In between {tool_call_start} and{tool_call_end} tags, you MUST respond in a valid JSON schema.\n",
    "    In between the {tool_call_start} and {tool_call_end} tags you MUST only write in json; no other text is allowed.\n",
    "\n",
    "    Rule 3: If user decides to run the function, they will output the result of the function call between the {tool_response_start} and {tool_response_start} tags. If it answers the user's question, you should incorporate the output of the function in your answer.\n",
    "\n",
    "\n",
    "    Here are the tools available to you:\n",
    "    {tool_list_start}\\n{tool_schema}\\n{tool_list_end}\n",
    "\n",
    "    Remember, don't make assumptions about what values to plug into functions. If you are missing the parameters to make a function call, please ask the user for the parameters. Do not be afraid to ask.\n",
    "    \"\"\"\n",
    "\n",
    "    return system_prompt.format(\n",
    "        tool_list_start=tool_list_start,\n",
    "        tool_list_end=tool_list_end,\n",
    "        tool_call_start=tool_call_start,\n",
    "        tool_call_end=tool_call_end,\n",
    "        tool_response_start=tool_response_start,\n",
    "        tool_response_end=tool_response_end,\n",
    "        tool_schema=tool_schema,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tool(text):\n",
    "    return \"<tool_call>\" in text and \"</tool_call>\" in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool(text, start_tag = \"<tool_call>\", end_tag = \"</tool_call>\"):\n",
    "\n",
    "\n",
    "\n",
    "    return function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool(text, start_tag, end_tag):\n",
    "    start_index = text.find(start_tag)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "\n",
    "    end_index = text.find(end_tag, start_index + len(start_tag))\n",
    "    if end_index == -1:\n",
    "        # if end_tag not present, check for last occurrence of start_tag and use this as the end_index\n",
    "        end_index = text.rfind(start_tag, start_index + len(start_tag))\n",
    "        if end_index == -1:\n",
    "            return None\n",
    "\n",
    "    function_call = text[start_index + len(start_tag):end_index].strip(\" \\n\")\n",
    "    return function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate  Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(mode, messages, regex_str, verbose=0):\n",
    "\n",
    "  if mode == \"vllm-endpoint\":\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"databricks/dbrx-instruct\",\n",
    "      max_tokens=350,\n",
    "      messages=messages,\n",
    "      extra_body=dict(guided_regex=regex_str, guided_decoding_backend=\"outlines\"),\n",
    "      )\n",
    "    raw_text = completion.choices[0].message.content\n",
    "  elif mode == \"transformers\":\n",
    "\n",
    "    if verbose >= 1:\n",
    "      print(\"Creating Generator Index...\", end=\"\\t\")\n",
    "\n",
    "    generator = generate.regex(model, regex_str)\n",
    "    generator.format_sequence = lambda x: x #json.loads(x)\n",
    "\n",
    "    if verbose >= 1:\n",
    "      print(\"Done\\nGenerating Text...\", end=\"\\t\")\n",
    "\n",
    "    rng_seed = 420\n",
    "    rng = torch.Generator(device=\"cuda\")\n",
    "    rng.manual_seed(rng_seed)\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    raw_text = generator(prompt, rng=rng, max_tokens=300)\n",
    "\n",
    "    if verbose >= 1:\n",
    "      print(\"Done\")\n",
    "\n",
    "  return raw_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex:\n",
      " (?:(?:({\"tool_name\": \"send_email\", \"tool_arguments\": \\{[\\n ]*\"sender\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"[\\n ]*,[\\n ]*\"recipient\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"[\\n ]*,[\\n ]*\"message\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"[\\n ]*\\}}){2}|({\"tool_name\": \"User\", \"tool_arguments\": \\{[\\n ]*\"name\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"[\\n ]*,[\\n ]*\"email\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"[\\n ]*\\}}){2})|({\"tool_name\": \"User\", \"tool_arguments\": \\{([\\n ]*\"name\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"([\\n ]*,[\\n ]*\"last_name\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\")?([\\n ]*,[\\n ]*\"id\"[\\n ]*:[\\n ]*(-)?(0|[1-9][0-9]*))?|([\\n ]*\"name\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"[\\n ]*,)?[\\n ]*\"last_name\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"([\\n ]*,[\\n ]*\"id\"[\\n ]*:[\\n ]*(-)?(0|[1-9][0-9]*))?|([\\n ]*\"name\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"[\\n ]*,)?([\\n ]*\"last_name\"[\\n ]*:[\\n ]*\"(?:[^\"\\\\\\x00-\\x1f\\x7f-\\x9f]|\\\\.)*\"[\\n ]*,)?[\\n ]*\"id\"[\\n ]*:[\\n ]*(-)?(0|[1-9][0-9]*))?[\\n ]*\\}}){2})\n",
      "\n",
      "Schemas:\n",
      " {\"properties\": {\"sender\": {\"title\": \"Sender\", \"type\": \"string\"}, \"recipient\": {\"title\": \"Recipient\", \"type\": \"string\"}, \"message\": {\"title\": \"Message\", \"type\": \"string\"}}, \"required\": [\"sender\", \"recipient\", \"message\"], \"title\": \"Arguments\", \"type\": \"object\"}\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"email\": {\"title\": \"Email\", \"type\": \"string\"}}, \"required\": [\"name\", \"email\"], \"title\": \"User\", \"type\": \"object\"}\n",
      "{ \"title\": \"User\", \"type\": \"object\", \"properties\": { \"name\": {\"type\": \"string\"}, \"last_name\": {\"type\": \"string\"}, \"id\": {\"type\": \"integer\"} } }\n"
     ]
    }
   ],
   "source": [
    "def send_email(sender: str, recipient: str, message: str):\n",
    "    return \"Hi\"\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "\n",
    "schema = \"\"\"\n",
    "{\n",
    "  \"title\": \"User\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"name\": {\"type\": \"string\"},\n",
    "    \"last_name\": {\"type\": \"string\"},\n",
    "    \"id\": {\"type\": \"integer\"}\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "tools = [send_email, User, schema]\n",
    "regex_str, tool_schema = tool_to_regex(tools, n_tool_calls=2)\n",
    "\n",
    "\n",
    "print(\"Regex:\\n\", regex_str, end=\"\\n\\n\")\n",
    "print(\"Schemas:\\n\", tool_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'You are a function calling AI model. Your job is to answer the user\\'s questions and you may call one or more functions to do this.\\n\\n\\n    Please use your own judgment as to whether or not you should call a function. In particular, you must follow these guiding principles:\\n    1. You may call one or more functions to assist with the user query.\\n    2. You do not need to call a function. If none of the functions can be used to answer the user\\'s question, please do not make the function call.\\n    3. Don\\'t make assumptions about what values to plug into functions. If you are missing the parameters to make a function call, please ask the user for the parameters.\\n    4. You may assume the user has implemented the function themselves.\\n    5. You may assume the user will call the function on their own. You should NOT ask the user to call the function and let you know the result; they will do this on their own.\\n\\n\\n    You can only call functions according the following formatting rules:\\n    Rule 1: All the functions you have access to are contained within <tool></tools> XML tags. You cannot use any functions that are not listed between these tags.\\n\\n    Rule 2: For each function call return a json object (using quotes) with function name and arguments within <tool_call>\\n{ }\\n</tool_call> XML tags as follows:\\n    * With arguments:\\n    <tool_call>\\n{\"tool_name\": \"function_name\", \"tool_arguments\": {\"argument_1_name\": \"value\", \"argument_2_name\": \"value\"} }\\n</tool_call>\\n    * Without arguments:\\n    <tool_call>\\n{ \"tool_name\": \"function_name\", \"tool_arguments\": {} }\\n</tool_call>\\n    In between <tool_call> and</tool_call> tags, you MUST respond in a valid JSON schema.\\n    In between the <tool_call> and </tool_call> tags you MUST only write in json; no other text is allowed.\\n\\n    Rule 3: If user decides to run the function, they will output the result of the function call between the <tool_response> and <tool_response> tags. If it answers the user\\'s question, you should incorporate the output of the function in your answer.\\n\\n\\n    Here are the tools available to you:\\n    <tool>\\n{\"properties\": {\"sender\": {\"title\": \"Sender\", \"type\": \"string\"}, \"recipient\": {\"title\": \"Recipient\", \"type\": \"string\"}, \"message\": {\"title\": \"Message\", \"type\": \"string\"}}, \"required\": [\"sender\", \"recipient\", \"message\"], \"title\": \"Arguments\", \"type\": \"object\"}\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"email\": {\"title\": \"Email\", \"type\": \"string\"}}, \"required\": [\"name\", \"email\"], \"title\": \"User\", \"type\": \"object\"}\\n{ \"title\": \"User\", \"type\": \"object\", \"properties\": { \"name\": {\"type\": \"string\"}, \"last_name\": {\"type\": \"string\"}, \"id\": {\"type\": \"integer\"} } }\\n</tools>\\n\\n    Remember, don\\'t make assumptions about what values to plug into functions. If you are missing the parameters to make a function call, please ask the user for the parameters. Do not be afraid to ask.\\n    '}\n",
      "{'role': 'user', 'content': 'And can you also make two user profiles, one for Alice (alice@gmail.com) and one for Bob (bob@databricks.com)?'}\n"
     ]
    }
   ],
   "source": [
    "# user_query = \"Can you send an email from Alice (alice@gmail.com) to Bob (bob@databricks.com) saying 'We cracked the code!'? And can you also make two user profiles, one for Alice and one for Bob?\"\n",
    "user_query = \"And can you also make two user profiles, one for Alice (alice@gmail.com) and one for Bob (bob@databricks.com)?\"\n",
    "\n",
    "system_prompt = get_system_prompt(tool_schema)\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_query}\n",
    "  ]\n",
    "\n",
    "for message in messages:\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Generator Index...\tDone\n",
      "Generating Text...\tDone\n",
      "{\"tool_name\": \"User\", \"tool_arguments\": {\n",
      "  \"name\": \"Alice\",\n",
      "  \"email\": \"alice@gmail.com\"\n",
      "}}{\"tool_name\": \"User\", \"tool_arguments\": {\n",
      "  \"name\": \"Bob\",\n",
      "  \"email\": \"bob@databricks.com\"\n",
      "}}\n"
     ]
    }
   ],
   "source": [
    "raw_text = generate_text(mode, messages, regex_str, verbose=1)\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tool_instances(raw_text):\n",
    "    \"\"\"Return a list of all tools that match the tool_regex. This works for multiple functions.\"\"\"\n",
    "\n",
    "    tool_regex = r'\\{\"tool_name\": \"([^\"]+)\", \"tool_arguments\": (\\{[^{}]*\\})\\}'\n",
    "    matches = re.findall(tool_regex, raw_text)\n",
    "    tool_instances = []\n",
    "    for match in matches:\n",
    "        tool_name = match[0]\n",
    "        tool_arguments = json.loads(match[1])\n",
    "        tool_instances.append({'tool_name': tool_name, 'tool_arguments': tool_arguments})\n",
    "    return tool_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tool_regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_name\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m([^\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]+)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_arguments\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: (\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m})\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(tool_regex, \u001b[43mraw_text\u001b[49m)\n\u001b[1;32m      4\u001b[0m matches\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_text' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "tool_regex = r'\\{\"tool_name\": \"([^\"]+)\", \"tool_arguments\": (\\{[^{}]*\\})\\}'\n",
    "matches = re.findall(tool_regex, raw_text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 4 column 3 (char 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tool \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tool\n",
      "File \u001b[0;32m/usr/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/usr/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 4 column 3 (char 92)"
     ]
    }
   ],
   "source": [
    "tool = json.loads(raw_text)\n",
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tool(\u001b[43mraw_text\u001b[49m):\n\u001b[1;32m      3\u001b[0m     tool \u001b[38;5;241m=\u001b[39m parse_tool(raw_text)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tool)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_text' is not defined"
     ]
    }
   ],
   "source": [
    "tool = None\n",
    "if is_tool(raw_text):\n",
    "    tool = parse_tool(raw_text)\n",
    "print(tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result(function_name, result):\n",
    "    # This method is used to format the result in a standard way.\n",
    "    args_string = ', '.join([f\"{key}='{value}'\" if isinstance(value, str) else f\"{key}={value}\" for key, value in result.items()])\n",
    "    # Creating the output string with the function name and arguments\n",
    "    output_string = f'[{function_name}({args_string})]'\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[send_email(sender='alice@gmail.com', recipient='bob@databricks.com', message='We cracked the code!')]\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_result(tool[\"tool_name\"], tool[\"tool_arguments\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gorilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
